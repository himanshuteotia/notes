# **Monitoring Kubernetes Autoscaling with AWS CloudWatch**

To monitor **Horizontal Pod Autoscaler (HPA)** and **Cluster Autoscaler (CA)** in AWS **EKS**, we will:
‚úÖ Enable **Kubernetes Metrics in CloudWatch**  
‚úÖ Set up **CloudWatch Container Insights**  
‚úÖ View **HPA and Node Scaling Metrics**  
‚úÖ Create **Alarms and Dashboards** for autoscaling events  

---

## **1Ô∏è‚É£ Step 1: Enable CloudWatch Metrics for EKS**
AWS **Container Insights** allows CloudWatch to collect Kubernetes **pod, node, and cluster** metrics.

### **Enable CloudWatch Logs for Your EKS Cluster**
Run:
```sh
aws eks update-cluster-config --name my-k8s-cluster --logging '{"clusterLogging":[{"types":["api", "audit", "authenticator", "controllerManager", "scheduler"],"enabled":true}]}' --region us-east-1
```
‚úÖ This enables **API, audit, authenticator, scheduler, and controller logs** in CloudWatch.

---

## **2Ô∏è‚É£ Step 2: Install CloudWatch Agent on Kubernetes**
AWS provides a **CloudWatch Agent** to push Kubernetes **HPA & Node metrics** to CloudWatch.

### **Create a CloudWatch IAM Policy**
Create a policy to allow Kubernetes to push logs:
```sh
aws iam create-policy --policy-name CloudWatchAgentServerPolicy --policy-document file://cloudwatch-policy.json
```

Create a file `cloudwatch-policy.json`:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "logs:PutLogEvents",
        "logs:DescribeLogStreams",
        "logs:DescribeLogGroups",
        "logs:CreateLogStream",
        "logs:CreateLogGroup",
        "cloudwatch:PutMetricData",
        "ec2:DescribeTags"
      ],
      "Resource": "*"
    }
  ]
}
```
Attach this policy to your **EKS worker node IAM role**:
```sh
aws iam attach-role-policy --role-name <your-node-role> --policy-arn arn:aws:iam::<your-account-id>:policy/CloudWatchAgentServerPolicy
```
‚úÖ **Check if the IAM policy is attached**:
```sh
aws iam list-attached-role-policies --role-name <your-node-role>
```

### **Deploy the CloudWatch Agent to EKS**
Create a namespace:
```sh
kubectl create namespace amazon-cloudwatch
```
Deploy the **CloudWatch Agent**:
```sh
kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/master/k8s-yaml-templates/cwagent-daemonset.yaml
```
‚úÖ Verify if the CloudWatch Agent is running:
```sh
kubectl get pods -n amazon-cloudwatch
```
You should see something like:
```
NAME                             READY   STATUS    RESTARTS   AGE
cloudwatch-agent-xyz123          1/1     Running   0          2m
```

---

## **3Ô∏è‚É£ Step 3: Enable HPA Metrics in CloudWatch**
Now, enable **Kubernetes Pod Autoscaling Metrics**.

### **Deploy CloudWatch Metrics Config**
Create a ConfigMap:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cwagent-config
  namespace: amazon-cloudwatch
data:
  cwagentconfig.json: |
    {
      "metrics": {
        "namespace": "ContainerInsights",
        "metrics_collected": {
          "kubernetes": {
            "cluster_name": "my-k8s-cluster",
            "metrics_collection_interval": 60
          }
        }
      }
    }
```
Apply it:
```sh
kubectl apply -f cwagent-config.yaml
```
‚úÖ **Restart the CloudWatch Agent**:
```sh
kubectl delete pod -n amazon-cloudwatch -l name=cloudwatch-agent
```

---

## **4Ô∏è‚É£ Step 4: View Kubernetes Metrics in CloudWatch**
Now, go to **AWS CloudWatch ‚Üí Metrics ‚Üí Container Insights**.

### **Available Metrics**
- **Node Scaling Metrics**:
  - `node_cpu_utilization`
  - `node_memory_utilization`
  - `node_filesystem_utilization`

- **Pod Scaling (HPA) Metrics**:
  - `pod_cpu_utilization`
  - `pod_memory_utilization`
  - `pod_network_tx_bytes`

‚úÖ **Verify HPA Scaling Metrics**:
```sh
kubectl get hpa
```
Expected Output:
```
NAME         REFERENCE         TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
nodejs-hpa   Deployment/nodejs-app   60%/50%   2         10        5          5m
```
‚û° CloudWatch will now **track pod replicas based on CPU/memory usage**.

---

## **5Ô∏è‚É£ Step 5: Create CloudWatch Dashboards for Autoscaling**
To visualize autoscaling in CloudWatch:

### **Create a Custom Dashboard**
1Ô∏è‚É£ **Go to AWS CloudWatch** ‚Üí **Dashboards**  
2Ô∏è‚É£ **Create a New Dashboard** (e.g., `EKS-Autoscaling`)  
3Ô∏è‚É£ **Add the Following Widgets**:
   - **Line Chart: `ClusterAutoscaler.NodeGroup.DesiredCapacity`**
   - **Line Chart: `ClusterAutoscaler.NodeGroup.RunningCapacity`**
   - **Bar Chart: `ClusterAutoscaler.NodeGroup.UnneededCapacity`**
   - **CPU/Memory Utilization for Pods**
   - **HPA Metrics: `pod_cpu_utilization`, `pod_memory_utilization`**

Now, you can monitor **HPA & Cluster Autoscaler scaling activity** in real time! üéØ

---

## **6Ô∏è‚É£ Step 6: Set CloudWatch Alarms for Scaling Events**
Set up **alerts** for when Kubernetes scales too aggressively.

### **Create an Alarm for High CPU Utilization**
1Ô∏è‚É£ Go to **AWS CloudWatch ‚Üí Alarms**  
2Ô∏è‚É£ Click **Create Alarm**  
3Ô∏è‚É£ Select **Metric ‚Üí Container Insights ‚Üí pod_cpu_utilization**  
4Ô∏è‚É£ Set a threshold (e.g., **80% CPU Usage**)  
5Ô∏è‚É£ Select **Action ‚Üí SNS Notification ‚Üí Email Alert**  
6Ô∏è‚É£ Click **Create Alarm**  

‚û° This will **alert you** when pod CPU usage exceeds 80%, so you can monitor HPA scaling.

---

## **7Ô∏è‚É£ Summary**
‚úÖ **Step 1:** Enable CloudWatch Logs for EKS  
‚úÖ **Step 2:** Install CloudWatch Agent in Kubernetes  
‚úÖ **Step 3:** Enable HPA Metrics Collection  
‚úÖ **Step 4:** View EKS Cluster Metrics in CloudWatch  
‚úÖ **Step 5:** Create CloudWatch Dashboards for Autoscaling  
‚úÖ **Step 6:** Set CloudWatch Alarms for High CPU Utilization  

---

## **üîπ Next Steps**
üîπ **Monitor logs in AWS CloudWatch Logs**  
üîπ **Set up Prometheus & Grafana for advanced monitoring**  
üîπ **Use AWS X-Ray for tracing EKS services**  


# **Monitor Logs in AWS CloudWatch Logs for EKS**
To effectively **monitor logs** in AWS **CloudWatch Logs** for your **EKS cluster**, follow these steps:

‚úÖ **Step 1:** Enable **EKS Logging to CloudWatch**  
‚úÖ **Step 2:** Deploy **Fluent Bit** to send logs to CloudWatch  
‚úÖ **Step 3:** Verify logs in **CloudWatch Log Groups**  
‚úÖ **Step 4:** Set up **Log Insights for Analysis**  
‚úÖ **Step 5:** Create **CloudWatch Alarms for Log Monitoring**  

---

## **1Ô∏è‚É£ Step 1: Enable EKS Logging in CloudWatch**
AWS allows **EKS control plane logs** to be sent to **CloudWatch Logs**.

Run the following command to enable logging:
```sh
aws eks update-cluster-config --name my-k8s-cluster \
  --logging '{"clusterLogging":[{"types":["api", "audit", "authenticator", "controllerManager", "scheduler"],"enabled":true}]}' \
  --region us-east-1
```
üìå **Control Plane Logs Available in CloudWatch:**
- **API Server Logs** ‚Üí Requests made to Kubernetes API
- **Audit Logs** ‚Üí Security and authentication logs
- **Scheduler Logs** ‚Üí Information on scheduling decisions
- **Controller Logs** ‚Üí Logs related to controllers running in Kubernetes

‚úÖ **Verify Logging is Enabled**:
```sh
aws eks describe-cluster --name my-k8s-cluster --query "cluster.logging"
```

---

## **2Ô∏è‚É£ Step 2: Deploy Fluent Bit to Collect Application Logs**
By default, **EKS logs control plane events**, but **application logs (Pods, Services, and Events)** must be manually forwarded.

### **Why Use Fluent Bit?**
Fluent Bit is a lightweight log processor that **sends Kubernetes logs to CloudWatch**.

### **Create IAM Policy for Fluent Bit**
Create a new **IAM policy**:
```sh
aws iam create-policy --policy-name FluentBitPolicy --policy-document file://fluentbit-policy.json
```
Create `fluentbit-policy.json`:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents",
                "logs:DescribeLogGroups",
                "logs:DescribeLogStreams"
            ],
            "Resource": "*"
        }
    ]
}
```
Attach this policy to the EKS worker node role:
```sh
aws iam attach-role-policy --role-name <your-node-role> --policy-arn arn:aws:iam::<your-account-id>:policy/FluentBitPolicy
```

### **Deploy Fluent Bit**
1Ô∏è‚É£ Create a **namespace** for Fluent Bit:
```sh
kubectl create namespace amazon-cloudwatch
```

2Ô∏è‚É£ **Deploy Fluent Bit ConfigMap:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: amazon-cloudwatch
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Daemon        Off
        Log_Level     info
    [INPUT]
        Name          tail
        Path          /var/log/containers/*.log
        Parser        docker
        Tag           kube.*
    [OUTPUT]
        Name          cloudwatch
        Match         kube.*
        region        us-east-1
        log_group_name fluent-bit-logs
        auto_create_group true
        log_stream_name_from_tag true
```
Apply the ConfigMap:
```sh
kubectl apply -f fluentbit-config.yaml
```

3Ô∏è‚É£ **Deploy Fluent Bit DaemonSet:**
```sh
kubectl apply -f https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/master/k8s-yaml-templates/fluent-bit-daemonset.yaml
```

‚úÖ **Check if Fluent Bit is running:**
```sh
kubectl get pods -n amazon-cloudwatch
```
You should see:
```
fluent-bit-abc123   1/1 Running
```

---

## **3Ô∏è‚É£ Step 3: Verify Logs in CloudWatch**
Now that **Fluent Bit is running**, check if logs are being sent.

### **View Logs in AWS CloudWatch**
1Ô∏è‚É£ **Go to AWS Console ‚Üí CloudWatch**  
2Ô∏è‚É£ Click **Logs ‚Üí Log Groups**  
3Ô∏è‚É£ Look for **fluent-bit-logs**  
4Ô∏è‚É£ Click on a log stream to view **real-time Kubernetes logs**.

---

## **4Ô∏è‚É£ Step 4: Use CloudWatch Logs Insights for Analysis**
To **search & filter logs**, use **CloudWatch Logs Insights**.

### **Example Queries**
üìå **Find all logs from a specific pod:**
```sql
fields @timestamp, @message
| filter kubernetes.pod_name = "nodejs-app-xyz"
| sort @timestamp desc
```
üìå **Find logs containing errors:**
```sql
fields @timestamp, @message
| filter @message like /error/
| sort @timestamp desc
```

üìå **Count errors by pod:**
```sql
fields kubernetes.pod_name, count(*) as error_count
| filter @message like /error/
| stats count(*) by kubernetes.pod_name
```
These queries help **debug application errors faster**.

---

## **5Ô∏è‚É£ Step 5: Create CloudWatch Alarms for Log Monitoring**
To **get alerts** when errors occur in logs:

### **Create a Log Filter**
1Ô∏è‚É£ Go to **AWS CloudWatch ‚Üí Logs ‚Üí Log Groups**  
2Ô∏è‚É£ Select **fluent-bit-logs**  
3Ô∏è‚É£ Click **Create Metric Filter**  
4Ô∏è‚É£ Enter **Filter Pattern:**
```sh
?error ?failed ?timeout
```
5Ô∏è‚É£ Click **Create Filter**  
6Ô∏è‚É£ Set a metric name like `ErrorCountMetric`  

### **Create an Alarm**
1Ô∏è‚É£ Go to **AWS CloudWatch ‚Üí Alarms**  
2Ô∏è‚É£ Click **Create Alarm**  
3Ô∏è‚É£ Select the **ErrorCountMetric** created above  
4Ô∏è‚É£ Set the threshold (e.g., **trigger if errors > 5 in 5 minutes**)  
5Ô∏è‚É£ Set **Notification** to **Send Email via SNS**  
6Ô∏è‚É£ Click **Create Alarm**  

‚úÖ **You will now receive email alerts if errors occur in Kubernetes logs!** üì©

---

## **6Ô∏è‚É£ Summary**
‚úÖ **Step 1:** Enable Kubernetes logging in CloudWatch  
‚úÖ **Step 2:** Deploy Fluent Bit to collect application logs  
‚úÖ **Step 3:** View logs in CloudWatch Log Groups  
‚úÖ **Step 4:** Use Log Insights to analyze logs  
‚úÖ **Step 5:** Set CloudWatch Alarms for error monitoring  

---

## **7Ô∏è‚É£ Next Steps**
üîπ **Use Prometheus & Grafana for real-time monitoring**  
üîπ **Set up AWS X-Ray for tracing EKS service requests**  
üîπ **Enable VPC Flow Logs to analyze EKS network traffic**  